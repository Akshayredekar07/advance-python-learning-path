{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Python Packages and Modules**\n",
    "\n",
    "### **Encapsulation Mechanism**\n",
    "- A **package** is a way to group related **modules** into a single unit.\n",
    "- A **package** is simply a **folder or directory** containing Python modules.\n",
    "- Any folder that contains an `__init__.py` file is considered a **Python package**.\n",
    "- A package can contain **sub-packages** as well.\n",
    "\n",
    "#### **Advantages of Using Packages**\n",
    "1. **Resolves naming conflicts** by organizing modules into different namespaces.\n",
    "2. **Uniquely identifies components** within an application.\n",
    "3. **Improves modularity** by structuring the code into reusable modules.\n",
    "\n",
    "\n",
    "\n",
    "### **Package Structure Example**\n",
    "```plaintext\n",
    "Loan/\n",
    "│── __init__.py\n",
    "│── HomeLoan/\n",
    "│   ├── __init__.py\n",
    "│   ├── x.py\n",
    "│   ├── y.py\n",
    "│── VehicleLoan/\n",
    "│   ├── __init__.py\n",
    "│   ├── m.py\n",
    "│   ├── n.py\n",
    "```\n",
    "- `Loan` is the main package.\n",
    "- It contains two sub-packages: `HomeLoan` and `VehicleLoan`.\n",
    "- Each sub-package has its own modules (`x.py`, `y.py`, etc.).\n",
    "- The presence of `__init__.py` in each folder makes it a **Python package**.\n",
    "\n",
    "\n",
    "\n",
    "### **Example 1: Creating and Importing a Package**\n",
    "#### **Directory Structure**\n",
    "```plaintext\n",
    "D:\\Python_classes\n",
    "│── test.py\n",
    "│── pack1/\n",
    "│   ├── __init__.py  # Empty file\n",
    "│   ├── module1.py\n",
    "```\n",
    "\n",
    "#### **Contents of `module1.py`**\n",
    "```python\n",
    "def f1():\n",
    "    print(\"Hello, this is from module1 present in pack1\")\n",
    "```\n",
    "\n",
    "#### **Contents of `test.py` (Version 1)**\n",
    "```python\n",
    "import pack1.module1\n",
    "pack1.module1.f1()\n",
    "```\n",
    "\n",
    "#### **Alternative Import Syntax (Version 2)**\n",
    "```python\n",
    "from pack1.module1 import f1\n",
    "f1()\n",
    "```\n",
    "\n",
    "### **Example 2: Using Nested Packages**\n",
    "#### **Directory Structure**\n",
    "```plaintext\n",
    "D:\\Python_classes\n",
    "│── test.py\n",
    "│── com/\n",
    "│   ├── __init__.py  # Empty file\n",
    "│   ├── module1.py\n",
    "│   ├── durgasoft/\n",
    "│       ├── __init__.py  # Empty file\n",
    "│       ├── module2.py\n",
    "```\n",
    "\n",
    "#### **Contents of `module1.py`**\n",
    "```python\n",
    "def f1():\n",
    "    print(\"Hello, this is from module1 present in com\")\n",
    "```\n",
    "\n",
    "#### **Contents of `module2.py`**\n",
    "```python\n",
    "def f2():\n",
    "    print(\"Hello, this is from module2 present in com.durgasoft\")\n",
    "```\n",
    "\n",
    "#### **Contents of `test.py`**\n",
    "```python\n",
    "from com.module1 import f1\n",
    "from com.durgasoft.module2 import f2\n",
    "\n",
    "f1()\n",
    "f2()\n",
    "```\n",
    "\n",
    "#### **Expected Output**\n",
    "```plaintext\n",
    "D:\\Python_classes> py test.py\n",
    "Hello, this is from module1 present in com\n",
    "Hello, this is from module2 present in com.durgasoft\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Python Code Structure**\n",
    "- A **library** contains multiple **packages**.\n",
    "- A **package** contains multiple **modules**.\n",
    "- A **module** contains **functions, classes, and variables**.\n",
    "\n",
    "#### **Diagram Representation**\n",
    "```plaintext\n",
    "Library\n",
    "│── pack1/\n",
    "│   ├── module1.py\n",
    "│   ├── module2.py\n",
    "│── pack2/\n",
    "│   ├── module1.py\n",
    "│   ├── module2.py\n",
    "│── packN/\n",
    "│   ├── moduleN.py\n",
    "```\n",
    "- Each **module** contains **functions, variables, and classes**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Case Study: Data Engineering Pipeline using Python Packages and Modules**  \n",
    "\n",
    "\n",
    "\n",
    "### **Problem Statement**  \n",
    "A **financial institution** wants to build an **automated credit risk assessment system**. The system should:  \n",
    "1. **Ingest data** from multiple sources (CSV, databases, APIs).  \n",
    "2. **Preprocess and clean the data** (handle missing values, normalize features).  \n",
    "3. **Perform feature engineering** (generate new features).  \n",
    "4. **Train an ML model** to classify loan applicants as **low risk or high risk**.  \n",
    "5. **Deploy the model** for real-time predictions.  \n",
    "\n",
    "\n",
    "\n",
    "### **Project Structure**  \n",
    "We organize our pipeline using **Python packages and modules** to ensure a modular and maintainable codebase.  \n",
    "\n",
    "```plaintext\n",
    "credit_risk_pipeline/\n",
    "│── main.py                      # Entry point of the pipeline\n",
    "│── config.py                     # Configuration settings\n",
    "│── data/\n",
    "│   ├── raw/                      # Raw data files\n",
    "│   ├── processed/                 # Processed data files\n",
    "│── src/\n",
    "│   ├── __init__.py               # Makes src a package\n",
    "│   ├── data_ingestion.py         # Module for data collection\n",
    "│   ├── data_preprocessing.py     # Module for cleaning data\n",
    "│   ├── feature_engineering.py    # Module for feature transformation\n",
    "│   ├── model_training.py         # Module for training ML model\n",
    "│   ├── model_inference.py        # Module for making predictions\n",
    "│── models/\n",
    "│   ├── trained_model.pkl         # Trained ML model\n",
    "│── requirements.txt              # Dependencies\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **1. Data Ingestion Module (`data_ingestion.py`)**  \n",
    "This module fetches data from **CSV, databases, and APIs** and saves it to the `data/raw/` directory.  \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self, data_path=\"data/raw/\"):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def load_csv(self, file_name):\n",
    "        \"\"\"Load data from a CSV file.\"\"\"\n",
    "        file_path = os.path.join(self.data_path, file_name)\n",
    "        return pd.read_csv(file_path)\n",
    "\n",
    "    def save_data(self, df, file_name):\n",
    "        \"\"\"Save data to CSV.\"\"\"\n",
    "        file_path = os.path.join(self.data_path, file_name)\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Data saved to {file_path}\")\n",
    "```\n",
    "\n",
    "**Usage in `main.py`**\n",
    "```python\n",
    "from src.data_ingestion import DataIngestion\n",
    "\n",
    "data_ingestor = DataIngestion()\n",
    "df = data_ingestor.load_csv(\"credit_data.csv\")\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **2. Data Preprocessing Module (`data_preprocessing.py`)**  \n",
    "Handles **missing values, normalization, and encoding**.  \n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "class DataPreprocessing:\n",
    "    def handle_missing_values(self, df):\n",
    "        \"\"\"Fill missing values with median.\"\"\"\n",
    "        return df.fillna(df.median())\n",
    "\n",
    "    def normalize_features(self, df, columns):\n",
    "        \"\"\"Normalize numerical features using StandardScaler.\"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        df[columns] = scaler.fit_transform(df[columns])\n",
    "        return df\n",
    "```\n",
    "\n",
    "**Usage in `main.py`**\n",
    "```python\n",
    "from src.data_preprocessing import DataPreprocessing\n",
    "\n",
    "preprocessor = DataPreprocessing()\n",
    "df_cleaned = preprocessor.handle_missing_values(df)\n",
    "df_normalized = preprocessor.normalize_features(df_cleaned, [\"income\", \"loan_amount\"])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **3. Feature Engineering Module (`feature_engineering.py`)**  \n",
    "Creates new features to improve model performance.  \n",
    "\n",
    "```python\n",
    "class FeatureEngineering:\n",
    "    def create_credit_ratio(self, df):\n",
    "        \"\"\"Create a new feature: Credit Utilization Ratio.\"\"\"\n",
    "        df[\"credit_ratio\"] = df[\"loan_amount\"] / df[\"income\"]\n",
    "        return df\n",
    "```\n",
    "\n",
    "**Usage in `main.py`**\n",
    "```python\n",
    "from src.feature_engineering import FeatureEngineering\n",
    "\n",
    "feature_engineer = FeatureEngineering()\n",
    "df_features = feature_engineer.create_credit_ratio(df_normalized)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **4. Model Training Module (`model_training.py`)**  \n",
    "Trains a **classification model** (e.g., Logistic Regression).  \n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "class ModelTraining:\n",
    "    def train_model(self, df, target_column):\n",
    "        \"\"\"Train a logistic regression model.\"\"\"\n",
    "        X = df.drop(columns=[target_column])\n",
    "        y = df[target_column]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Save model\n",
    "        with open(\"models/trained_model.pkl\", \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "        print(\"Model trained and saved!\")\n",
    "```\n",
    "\n",
    "**Usage in `main.py`**\n",
    "```python\n",
    "from src.model_training import ModelTraining\n",
    "\n",
    "trainer = ModelTraining()\n",
    "trainer.train_model(df_features, \"risk_category\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **5. Model Inference Module (`model_inference.py`)**  \n",
    "Loads the trained model and makes predictions.  \n",
    "\n",
    "```python\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "class ModelInference:\n",
    "    def __init__(self, model_path=\"models/trained_model.pkl\"):\n",
    "        with open(model_path, \"rb\") as f:\n",
    "            self.model = pickle.load(f)\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"Make predictions on new data.\"\"\"\n",
    "        return self.model.predict(df)\n",
    "```\n",
    "\n",
    "**Usage in `main.py`**\n",
    "```python\n",
    "from src.model_inference import ModelInference\n",
    "\n",
    "inference = ModelInference()\n",
    "predictions = inference.predict(df_features)\n",
    "print(predictions)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "1. **Modular Design**:  \n",
    "   - Each functionality is implemented in a separate module, making the code **organized and reusable**.  \n",
    "\n",
    "2. **Scalability**:  \n",
    "   - New data sources, transformations, or models can be added without changing the entire pipeline.  \n",
    "\n",
    "3. **Reusability**:  \n",
    "   - The `data_preprocessing.py` and `feature_engineering.py` modules can be reused in different projects.  \n",
    "\n",
    "4. **Separation of Concerns**:  \n",
    "   - **Data ingestion, processing, feature engineering, training, and inference** are clearly separated.  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
